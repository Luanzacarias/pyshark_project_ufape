
# ğŸš€ Projeto de AnÃ¡lise de Dados com Apache Spark

Este projeto demonstra como executar um cluster Apache Spark com Docker e Docker Compose, realizando anÃ¡lise distribuÃ­da de um dataset CSV usando PySpark.

## ğŸ—ï¸ Arquitetura do Projeto

- ğŸ”¹ 1 Spark Master
- ğŸ”¸ 3 Spark Workers
- ğŸŸ¢ 1 Spark Client (para execuÃ§Ã£o dos scripts)

## ğŸ“‚ Estrutura de DiretÃ³rios

```
.
â”œâ”€â”€ app/            # Scripts em PySpark
â”‚   â””â”€â”€ script-netflix.py
â”‚   â””â”€â”€ script.py
â”œâ”€â”€ dados/          # Dataset CSV
â”‚   â””â”€â”€ netflix_titles.csv
â”‚   â””â”€â”€ yellow_tripdata_2015.csv
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
â””â”€â”€ run_spark.sh
```

## â–¶ï¸ Passos para Executar

### 1ï¸âƒ£ Clone o Projeto

```bash
git clone git@github.com:Luanzacarias/pyshark_project_ufape.git
cd pyshark_project_ufape
```

### 2ï¸âƒ£ Adicione o Dataset

Utilizamos o arquivo "yellow_tripdata_2015-01.csv" disponÃ­vel no Kaggle [aqui](https://www.kaggle.com/datasets/elemento/nyc-yellow-taxi-trip-data), contÃ©m dados das viagens feitas por tÃ¡xis em New York no ano de 2015 no mÃªs de Janeiro. AlÃ©m dele, ainda usamos um base de dados dos tÃ­tulos dos filmes e sÃ©ries da Netflix tambÃ©m no Kaggle, disponÃ­vel [aqui](https://www.kaggle.com/datasets/shivamb/netflix-shows?resource=download)

```
dados/netflix_titles.csv ~ 3,15MB
dados/yellow_tripdata_2015-01.csv ~ 2GB
```


### 3ï¸âƒ£ Suba os Containers

```bash
docker-compose up -d
```

### 4ï¸âƒ£ Rode o script shell

```bash
./run_spark.sh
```

O resultado serÃ¡ exibido diretamente no terminal e salvo em um arquivo `metricas.txt`.

---

## ğŸŒ Interfaces Web

- **Spark Master UI:** [http://localhost:8080](http://localhost:8080)

Aqui vocÃª pode visualizar os workers conectados, jobs em execuÃ§Ã£o e status do cluster.

---

## â›” Encerrando o Cluster

```bash
docker-compose down
```

---

## ğŸ§  ReferÃªncias

- [Apache Spark Docker Hub](https://hub.docker.com/_/spark)
- [DocumentaÃ§Ã£o do Apache Spark](https://spark.apache.org/docs/latest/)

---
