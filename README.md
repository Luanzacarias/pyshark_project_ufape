
# ğŸš€ Projeto de AnÃ¡lise de Dados com Apache Spark

Este projeto demonstra como executar um cluster Apache Spark com Docker e Docker Compose, realizando anÃ¡lise distribuÃ­da de um dataset CSV usando PySpark.

## ğŸ—ï¸ Arquitetura do Projeto

- ğŸ”¹ 1 Spark Master
- ğŸ”¸ 3 Spark Workers
- ğŸŸ¢ 1 Spark Client (para execuÃ§Ã£o dos scripts)

## ğŸ“‚ Estrutura de DiretÃ³rios

```
.
â”œâ”€â”€ app/            # Scripts em PySpark
â”‚   â””â”€â”€ script.py
â”œâ”€â”€ dados/          # Dataset CSV
â”‚   â””â”€â”€ exemplo.csv
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## â–¶ï¸ Passos para Executar

### 1ï¸âƒ£ Clone o Projeto

```bash
git clone git@github.com:Luanzacarias/pyshark_project_ufape.git
cd pyshark_project_ufape
```

### 2ï¸âƒ£ Adicione seu Dataset

Coloque seu arquivo CSV dentro da pasta `/dados`. Exemplo:

```
dados/exemplo.csv
```

### 3ï¸âƒ£ Suba os Containers

```bash
docker-compose up -d
```

### 4ï¸âƒ£ Acesse o Container Cliente

```bash
docker exec -it <container-id:spark-client> bash
```

### 5ï¸âƒ£ Execute seu Script PySpark

```bash
/opt/spark/bin/spark-submit /app/script.py
```

O resultado serÃ¡ exibido diretamente no terminal.

---

## ğŸŒ Interfaces Web

- **Spark Master UI:** [http://localhost:8080](http://localhost:8080)

Aqui vocÃª pode visualizar os workers conectados, jobs em execuÃ§Ã£o e status do cluster.

---

## â›” Encerrando o Cluster

```bash
docker-compose down
```

---

## ğŸ§  ReferÃªncias

- [Apache Spark Docker Hub](https://hub.docker.com/_/spark)
- [DocumentaÃ§Ã£o do Apache Spark](https://spark.apache.org/docs/latest/)

---
